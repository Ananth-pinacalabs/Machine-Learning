{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVx88xjCX5CrXQDgPL3Tkj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananth-pinacalabs/Machine-Learning/blob/main/pytorch/pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align = \"center\"><b>Pytorch crash course</b></h1>"
      ],
      "metadata": {
        "id": "U121Ol8401i1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following  is a notebook created   by following and coding along the video <a href = \"https://youtu.be/OIenNRt2bjg?si=xmAPi3_VMLWqod0i\" alt = \"link to youtube video\"> PyTorch Crash Course - Getting Started with Deep Learning</a>\n",
        "by AssemblyAI. please check out the video!"
      ],
      "metadata": {
        "id": "ItjUqp0B0SCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align = \"center\"><b> Overview </b></h2>\n",
        "\n",
        "The notebook follows the following structure, covering the enlisted topics in the listed order.\n",
        "\n",
        "* Tensor Basics\n",
        "* Autograd\n",
        "* Training loop: model, Loss, & Optimizer.\n",
        "* Neural Network\n",
        "* convolutional Network with pytorch.\n"
      ],
      "metadata": {
        "id": "VSGZDUt_q5O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "5jEYySDTrkPG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Understanding Tensors**"
      ],
      "metadata": {
        "id": "plpd0dwVuM5G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmDZSgOZqyzv",
        "outputId": "d51f61a9-bcd3-43c7-dc91-5c9e37320f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scalar: \n",
            "\n",
            "tensor([-3.3540e+36])\n",
            "\n",
            "vector:\n",
            "\n",
            "tensor([[ 1.7511e+38,  3.3348e-41,  7.0065e-44,  6.7871e+32, -6.7169e-10]])\n",
            "\n",
            "matrix:\n",
            "\n",
            "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 1.4013e-45]],\n",
            "\n",
            "        [[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00]]])\n",
            "random:\n",
            "\n",
            "tensor([[0.5516, 0.4826, 0.5740],\n",
            "        [0.5268, 0.2737, 0.0377],\n",
            "        [0.4083, 0.0869, 0.9821],\n",
            "        [0.2937, 0.5923, 0.6108],\n",
            "        [0.3585, 0.9355, 0.4061]])\n",
            "\n",
            "ones:\n",
            "\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "\n",
            "zeros:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# torch.empty(size) returns uninitialised tensors\n",
        "\n",
        "print(\"scalar:\", \"\\n\")\n",
        "\n",
        "print(torch.empty(1))\n",
        "print(\"\\nvector:\\n\")\n",
        "print(torch.empty(1, 5))\n",
        "print(\"\\nmatrix:\\n\")\n",
        "print(torch.empty(2, 3, 3))\n",
        "\n",
        "# torch.ones(size)\n",
        "# torch.zeros(size)\n",
        "# torch.rand(size)  - > torch  with random numbers btwn [0, 1]\n",
        "\n",
        "\n",
        "print(\"random:\\n\")\n",
        "print(torch.rand(5, 3))\n",
        "\n",
        "print(\"\\nones:\\n\")\n",
        "print(torch.ones(5, 3))\n",
        "print(\"\\nzeros:\\n\")\n",
        "torch.zeros(5, 3)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(2, 3)\n",
        "print(x.dtype) # the default dtype is a float 32.\n",
        "x = torch.ones(2, 3, dtype = torch.float16) # maually setting the datatype.\n",
        "print(x.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MR1aYHCdrhlz",
        "outputId": "0837d8d4-2d55-4252-887f-b492f9ffbb5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  get a tensor from other  datatypes.\n",
        "torch.tensor([1, 3]) # tensor from a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZY22IoDuZJS",
        "outputId": "5de0a0c1-9cc1-4c48-d278-f1eb815824d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the `requires_grad` parameter.\n",
        "This tensor would be addedto the computational graph when the gradient is being calculated"
      ],
      "metadata": {
        "id": "8cqcvMhLwH9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1, 2, 2, 1], dtype = torch.float32, requires_grad = True )\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnzdCuBJvXdp",
        "outputId": "1f2ae41b-15a5-4860-ed06-4491727bac3f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 2., 1.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Arithmatic operations on tensors**"
      ],
      "metadata": {
        "id": "eGISy0eex9Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "x = torch.tensor([1.])\n",
        "y = torch.tensor([2.])\n",
        "print(\"x: \", x, \"\\ny: \", y)\n",
        "\n",
        "print(\"\\nusing opertor: \", x+y)\n",
        "print(\"using function: \", torch.add(x, y))\n",
        "\n",
        "# inplace operation.\n",
        "y.add_(x)\n",
        "print(\"\\ny after inplace operation:\", y)\n",
        "\n",
        "\n",
        "# other operation\n",
        "print(\"\\nother operations:\")\n",
        "print(\"subtraction: \", torch.subtract(x, y))\n",
        "print(\"multiplication: \", torch.mul(x, y))\n",
        "print(\"division: \", torch.div(x, y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0HR2naFwkys",
        "outputId": "4f339c9f-d022-431d-fdf2-8c96110897c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:  tensor([1.]) \n",
            "y:  tensor([2.])\n",
            "\n",
            "using opertor:  tensor([3.])\n",
            "using function:  tensor([3.])\n",
            "\n",
            "y after inplace operation: tensor([3.])\n",
            "\n",
            "other operations:\n",
            "subtraction:  tensor([-2.])\n",
            "multiplication:  tensor([3.])\n",
            "division:  tensor([0.3333])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.randn(4, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFYLyPqrxNCu",
        "outputId": "2041beda-5520-4d39-ba0a-c7d85a946acd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0033,  0.6548,  0.2890,  1.2536],\n",
              "        [ 0.5166,  0.4802,  0.1275, -1.0205],\n",
              "        [ 1.0178,  0.4333, -1.2922,  1.1745],\n",
              "        [-2.1423, -1.4233, -1.8409,  1.2337]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Reshape tensor with tensor.view()**"
      ],
      "metadata": {
        "id": "hWVEXW2nyqUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(4, 4)\n",
        "print(x.view(16), \"\\n\")\n",
        "print(x.reshape(-1, 8), \"\\n\")\n",
        "print(x.shape, x.size())# two different ways to get the shape of the tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJfuekf9xwQw",
        "outputId": "03687573-7d06-404f-cf8e-c603f4e4685b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.1197, -0.7843, -0.6337,  0.9130,  0.3418, -0.2074,  2.7107, -0.4821,\n",
            "         1.1647,  0.1048, -0.6128,  0.8392, -0.9973, -0.5111,  0.9537, -0.7123]) \n",
            "\n",
            "tensor([[ 0.1197, -0.7843, -0.6337,  0.9130,  0.3418, -0.2074,  2.7107, -0.4821],\n",
            "        [ 1.1647,  0.1048, -0.6128,  0.8392, -0.9973, -0.5111,  0.9537, -0.7123]]) \n",
            "\n",
            "torch.Size([4, 4]) torch.Size([4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Tensor from numpy and vise-versa**"
      ],
      "metadata": {
        "id": "ieYgWMhkzX3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.ones((1, 2))\n",
        "\n",
        "# Two ways to crete tensor from numpy array.\n",
        "y = torch.from_numpy(x)\n",
        "z = torch.tensor(x)\n",
        "print(\"using from_numpy() method: \", y)\n",
        "print(\"using tensor() method: \", z)\n",
        "\n",
        "# one problem tensor created using the from_tensor method will use the same memory locations as the numpy.\n",
        "# modifying the numpy will change the tensor.\n",
        "print()\n",
        "x+=1\n",
        "print(\"y after modification: \", y)\n",
        "print(\"z after modification: \", z)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZRd7_UWzHqi",
        "outputId": "60c1cc65-c47d-483d-c87b-973cba70999d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using from_numpy() method:  tensor([[1., 1.]], dtype=torch.float64)\n",
            "using tensor() method:  tensor([[1., 1.]], dtype=torch.float64)\n",
            "\n",
            "y after modification:  tensor([[2., 2.]], dtype=torch.float64)\n",
            "z after modification:  tensor([[1., 1.]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **GPU support**"
      ],
      "metadata": {
        "id": "0oX1o4wH0IH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default all the tensors  are created on the cpu but you can move them  to the gpu.\n"
      ],
      "metadata": {
        "id": "NnSVVQjI6G8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available()  else 'cpu')\n",
        "\n",
        "x = torch.rand(2, 3).to(device) # moving the tensor to the gpu.\n",
        "x = torch.rand(2, 2, device = device) # directly create the tensor on the gpu"
      ],
      "metadata": {
        "id": "S-qMuHT55o0B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Autograd</h3>\n",
        "\n",
        "\n",
        "The autograd package provides automatic differentiation on  Tensors. Generally speaking *torch.autograd* is  an engine for computing the vector-jacobian product. It computes the partial derivatives while applying the chain rule.\n"
      ],
      "metadata": {
        "id": "rVImncpi69K3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It tracks all the operations performed on the tensors that have `requires_grad` set to true and computes  the partial differentails for all these tensors."
      ],
      "metadata": {
        "id": "j3FMyQd89QPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>IMPORTANT</b></h3>\n",
        "\n",
        "\n",
        "so whenever you want to perform operations on these tensors, that you do not want to track the gradients for you must remove/ detach the vector from computational graph and perform the operation."
      ],
      "metadata": {
        "id": "ht_WVYIP-TPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad = True)\n",
        "y = x + 2\n",
        "z = y.mean()\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)\n",
        "\n",
        "\n",
        "print(\"\\nThe gradient functions\")\n",
        "print(y.grad_fn)\n",
        "print(z.grad_fn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWlXW6aO_TLm",
        "outputId": "1b19b3a1-cc15-4f7d-88cc-a3ecee8865be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.5649, -1.0556,  0.9222], requires_grad=True)\n",
            "tensor([2.5649, 0.9444, 2.9222], grad_fn=<AddBackward0>)\n",
            "tensor(2.1438, grad_fn=<MeanBackward0>)\n",
            "\n",
            "The gradient functions\n",
            "<AddBackward0 object at 0x7a3d0d2ca740>\n",
            "<MeanBackward0 object at 0x7a3d0d2cad10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Computing gradients and backpopagartion </h2>"
      ],
      "metadata": {
        "id": "1BOkpZlwDXqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now we have a gradient function associated with  the tensors in the computational graph.\n",
        "\n",
        "* Now when you perform back propogation you compute the gradients  some specific tensor.\n",
        "\n",
        "Lets say you want to backpropogate z then you will compute the partial differntial.\n",
        "<br>\n",
        "<br>\n",
        "$$\n",
        " \\text{partial differential wrt y}  = \\frac{\\partial z}{\\partial y}  \n",
        "$$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\text{partial differential wrt x} = \\frac{\\partial z}{\\partial x}\n",
        "$$"
      ],
      "metadata": {
        "id": "vh2WHMjLAISO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the .grad attribute tracks the gradient values  with back propagation.\n",
        "# When we call the backward() method then all the gradients are  calculated wrt that value.\n",
        "# the .grad a\n",
        "print(x.grad)\n",
        "z.backward()\n",
        "print(x.grad)\n",
        "\n",
        "# the gradients keep accumulating in the .grad  attribute each time you call the  backward method\n",
        "\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toqr-c63_ihu",
        "outputId": "d3d4dcdc-ef6b-4a4d-9c5e-e22ec1fae6db"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor([0.3333, 0.3333, 0.3333])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `.grad` attribute accumulates the gradients\n",
        " as .backward() is called repeatedly every epoch and  you need to empty these by calling maybe using optimizer.zero_grad()."
      ],
      "metadata": {
        "id": "_tXnuGTdFOea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some computations need to be peformed without gradients. like the weight update step after computing the gradients.  you could change this the following ways -\n",
        "\n",
        "* x.requires_grad_(False)\n",
        "* x.detach()\n",
        "* wrap in with torch.no_grad()\n",
        ""
      ],
      "metadata": {
        "id": "IOhDwy2aOOPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `.requires_grad()`"
      ],
      "metadata": {
        "id": "9UUjvtF4Pqhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(2, 2)\n",
        "b = (a * a).sum()\n",
        "print(a.requires_grad)\n",
        "print(b.grad_fn)\n",
        "\n",
        "a.requires_grad_(True) # inplace\n",
        "b = (a * a).sum()\n",
        "print(a.requires_grad)\n",
        "print(b.grad_fn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AleUTy3JHgS",
        "outputId": "29c67be0-3732-4676-bebc-515b869cc363"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "None\n",
            "True\n",
            "<SumBackward0 object at 0x7a3d0d2cb700>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cfahoeoCQW_w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(2, 2, requires_grad= True)\n",
        "b = a.detach()\n",
        "\n",
        "print(a.requires_grad)\n",
        "print(b.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeB2w6utPiy8",
        "outputId": "21e9395f-c382-4ba1-db71-07cda71dd90f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(2, 2, requires_grad= True)\n",
        "print(a.requires_grad)\n",
        "with torch.no_grad():\n",
        "  b = a ** 2\n",
        "  print(b.requires_grad)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XdatzqUQKQi",
        "outputId": "e5e4e84d-921a-4d15-8c3e-76d489f434a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Descent Autograd**"
      ],
      "metadata": {
        "id": "6mK-LV-wQnPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "linear regression `f(x) = 2x`"
      ],
      "metadata": {
        "id": "stnOVsfvw287"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1, 2, 3, 4, 5, 6, 7],  dtype = torch.float32)\n",
        "y = 2*a\n",
        "\n",
        "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)\n",
        "\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred - y)**2).mean()\n",
        "\n",
        "x_test = 5.0\n"
      ],
      "metadata": {
        "id": "9pXFNmoxQkfo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the model hyperparameters\n",
        "learning_rate = 0.01\n",
        "n_epochs = 100\n",
        "\n",
        "# running the training for 100 epochs\n",
        "for epoch in range(n_epochs):\n",
        "  y_pred = forward(x)\n",
        "  l = loss(y, y_pred)\n",
        "  l.backward()\n",
        "\n",
        "# updating the weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  w.grad.zero_()\n",
        "\n",
        "# printing verbose\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    print(\"epoch {0} w = {1}  loss = {2}\".format(epoch + 1, w.item(), l.item()))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osH0mIXUxPmo",
        "outputId": "57ab374c-70e6-45fb-8786-0b82ada0f401"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 10 w = 1.9999269247055054  loss = 2.9671917900486733e-07\n",
            "epoch 20 w = 1.9999995231628418  loss = 9.46036912002901e-12\n",
            "epoch 30 w = 2.0  loss = 0.0\n",
            "epoch 40 w = 2.0  loss = 0.0\n",
            "epoch 50 w = 2.0  loss = 0.0\n",
            "epoch 60 w = 2.0  loss = 0.0\n",
            "epoch 70 w = 2.0  loss = 0.0\n",
            "epoch 80 w = 2.0  loss = 0.0\n",
            "epoch 90 w = 2.0  loss = 0.0\n",
            "epoch 100 w = 2.0  loss = 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"test output :\", forward(x_test))\n",
        "print(\"final weights: \", w)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsjttF2-zWb8",
        "outputId": "7d5e036b-a769-482c-c4f4-cdd4dc73b25c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test output : tensor(10., grad_fn=<MulBackward0>)\n",
            "final weights:  tensor(2., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **model, loss, optimizer**\n",
        "A typical pytorch pipeline looks like this:\n",
        "\n",
        "1. Design a model\n",
        "2. Construct the loss and optimizer\n",
        "3. Training loop\n",
        "    * **Forward** - compute prediction and loss\n",
        "    * **Backward** - Compute gradients.  \n",
        "    * Update weights.\n",
        "    "
      ],
      "metadata": {
        "id": "q5HUyqko05po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Building the Linear Regression with pytorch modules.**"
      ],
      "metadata": {
        "id": "nAh6YUM61mRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Linear regression\n",
        "# f = w * x\n",
        "# here : f = 2 * x\n",
        "\n",
        "# 0) Training samples, watch the shape!\n",
        "x = torch.tensor([[1], [2], [3], [4], [5], [6], [7], [8]], dtype=torch.float32)\n",
        "y = torch.tensor([[2], [4], [6], [8], [10], [12], [14], [16]], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = x.shape\n",
        "print(f'n_samples = {n_samples}, n_features = {n_features}')\n",
        "\n",
        "# 0) create a test sample\n",
        "X_test = torch.tensor([5], dtype=torch.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th-M0o8K4ey_",
        "outputId": "cadb12e9-e537-4ab3-fe39-6d741c0268cc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_samples = 8, n_features = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class  LinearRegression(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LinearRegression, self).__init__()\n",
        "    self.lin  =  nn.Linear(input_dim, output_dim)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "input_size, output_size = n_features, n_features\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_epochs = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  # forward\n",
        "  y_pred = model(x)\n",
        "\n",
        "  # loss\n",
        "  l = loss(y, y_pred)\n",
        "\n",
        "  # computing the gradients\n",
        "  l.backward()\n",
        "\n",
        "  # updating the paramters\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero the gradients after updating the parameters.\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    w , b = model.parameters() # unpacking the parameters\n",
        "    print(\"epoch: \", epoch + 1, \"w:\", w[0][0].item(), \"loss: \", l.item())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbmciGW70IH9",
        "outputId": "c27ed7ec-41af-42a9-cff9-67b0dda9a3a1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  10 w: 2.069911241531372 loss:  0.033120978623628616\n",
            "epoch:  20 w: 2.068133592605591 loss:  0.03046197071671486\n",
            "epoch:  30 w: 2.065462350845337 loss:  0.028119731694459915\n",
            "epoch:  40 w: 2.0628952980041504 loss:  0.025957679376006126\n",
            "epoch:  50 w: 2.060429096221924 loss:  0.023961806669831276\n",
            "epoch:  60 w: 2.0580594539642334 loss:  0.02211938239634037\n",
            "epoch:  70 w: 2.0557825565338135 loss:  0.02041861228644848\n",
            "epoch:  80 w: 2.0535950660705566 loss:  0.01884862221777439\n",
            "epoch:  90 w: 2.0514934062957764 loss:  0.017399374395608902\n",
            "epoch:  100 w: 2.0494742393493652 loss:  0.016061486676335335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Linear regression\n",
        "# f = w * x\n",
        "# here : f = 2 * x\n",
        "\n",
        "# 0) Training samples, watch the shape!\n",
        "X = torch.tensor([[1], [2], [3], [4], [5], [6], [7], [8]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8], [10], [12], [14], [16]], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(f'n_samples = {n_samples}, n_features = {n_features}')\n",
        "\n",
        "# 0) create a test sample\n",
        "X_test = torch.tensor([5], dtype=torch.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erOdBCj169zG",
        "outputId": "de6fe6e1-724e-4591-f119-24113537351c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_samples = 8, n_features = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Design Model, the model has to implement the forward pass!\n",
        "\n",
        "# Here we could simply use a built-in model from PyTorch\n",
        "# model = nn.Linear(input_size, output_size)\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        # define different layers\n",
        "        self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "input_size, output_size = n_features, n_features\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "print(f'Prediction before training: f({X_test.item()}) = {model(X_test).item():.3f}')\n",
        "\n",
        "# 2) Define loss and optimizer\n",
        "learning_rate = 0.01\n",
        "n_epochs = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 3) Training loop\n",
        "for epoch in range(n_epochs):\n",
        "    # predict = forward pass with our model\n",
        "    y_predicted = model(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_predicted)\n",
        "\n",
        "    # calculate gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero the gradients after updating\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        w, b = model.parameters() # unpack parameters\n",
        "        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l.item())\n",
        "\n",
        "print(f'Prediction after training: f({X_test.item()}) = {model(X_test).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHY0wAN5BaCq",
        "outputId": "a05a6117-3f64-4336-eff0-4fcbb10ebb90"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5.0) = 1.363\n",
            "epoch  10 : w =  2.078075647354126  loss =  0.04108716547489166\n",
            "epoch  20 : w =  2.075925350189209  loss =  0.037827540189027786\n",
            "epoch  30 : w =  2.072948694229126  loss =  0.03491901978850365\n",
            "epoch  40 : w =  2.0700881481170654  loss =  0.032234083861112595\n",
            "epoch  50 : w =  2.0673396587371826  loss =  0.0297556109726429\n",
            "epoch  60 : w =  2.0646989345550537  loss =  0.027467746287584305\n",
            "epoch  70 : w =  2.062161922454834  loss =  0.025355709716677666\n",
            "epoch  80 : w =  2.0597243309020996  loss =  0.023406121879816055\n",
            "epoch  90 : w =  2.057382345199585  loss =  0.021606450900435448\n",
            "epoch  100 : w =  2.0551321506500244  loss =  0.019945137202739716\n",
            "Prediction after training: f(5.0) = 9.966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OacUUthPBgKC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}